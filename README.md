# Reffy

Reffy is your **W3C spec dependencies exploration companion**. It features a short set of tools to study term definitions and references found in W3C specifications.

## How to use

To launch the crawler and the report study tool, follow these steps:

0. Pre-requisites: [Git](https://git-scm.com/), [Node.js](https://nodejs.org/en/), a [W3C account](https://www.w3.org/accounts/request) and an [API key](https://www.w3.org/users/myprofile/apikeys) for the W3C API.
1. Clone the repository: `git clone git@github.com:tidoust/reffy.git`
2. From the root folder of reffy, install required dependencies: `npm install`
3. Create a `config.json` file, initialized with `{ "w3cApiKey": [API key] }`
3. Run the crawler: `node crawl-specs.js ./specs.json results.json`
4. Once done, run the study tool: `node study-specs.js ./results.json [true]` (pass `true` to create a report per specification instead of a report per anomaly). You may want to redirect the output to a file, e.g. using `node study-specs.js ./results.json > report.md`

Some notes:

* The crawler may take some time
* The crawler uses a local cache for HTTP exchanges. It will create and fill a `cache` subfolder in particular.
* The `./` prefix is needed to point the crawler and study tools at local files for the time being (one of the many things to improve in the code!)

## Reffy's tools

### Specs crawler

**Reffy's crawler** takes an initial list of spec URLs as input and generates a machine-readable report with facts about each spec, including:

1. Generic information such as the title of the spec or the URL of the Editor's Draft. This information is typically extracted from the [W3C API](https://w3c.github.io/w3c-api/).
2. The list of normative/informative references found in the spec.
3. Extended information about WebIDL term definitions and references that the spec contains

### Study tool

**Reffy's report study tool** takes the machine-readable report generated by the crawler, and creates a human-readable Markdown report of *potential* anomalies found in the report, such as:

1. specs that do not seem to reference any other spec normatively;
2. specs that define WebIDL terms but do not normatively reference the WebIDL spec;
3. specs that contain invalid WebIDL terms definitions;
4. specs that define WebIDL terms that are *also* defined in another spec;
5. specs that use WebIDL terms defined in another spec without referencing that spec normatively;
6. specs that use WebIDL terms for which the crawler could not find any definition in any of the specs it studied.

### WebIDL terms explorer

See the related [WebIDLPedia](https://dontcallmedom.github.io/webidlpedia) project and its [repo](https://github.com/dontcallmedom/webidlpedia).

### Other tools

Some of the tools used by the crawler may also be used directly.

The **references parser** takes the URL of a spec as input and generates a JSON structure that lists the normative and informative references found in the spec. To run the references parser: `node parse-references.js [url]`

The **WebIDL extractor** takes the URL of a spec as input and outputs the IDL definitions found in the spec as one block of text. To run the extractor: `node extract-webidl.js [url]`

The **WebIDL parser** takes the URL of a spec as input and generates a JSON structure that describes WebIDL term definitions and references that the spec contains. The parser uses [WebIDL2](https://github.com/darobin/webidl2.js/) to parse the WebIDL content found in the spec. To run the WebIDL parser: `node parse-webidl.js [url]`


For instance:

```bash
node parse-references.js https://w3c.github.io/presentation-api/
node extract-webidl.js https://www.w3.org/TR/webrtc/
node parse-webidl.js https://fetch.spec.whatwg.org/
```

## Technical notes

**Reffy is at an early stage of development and is not stable**. It should be able to parse most of the W3C/WHATWG specifications (both published versions and Editor's Drafts) that define WebIDL terms. The tool may work with other types of specs, but has not been tested with any of it.

There are a few exceptions to the rule, though. Notably, the crawler fails to parse Editor's Drafts of some specs that use [ReSpec](https://github.com/w3c/respec) because ReSpec uses `DOMParser` to normalize paddings in some cases and this does not yet work well with [jsdom](https://github.com/tmpvar/jsdom) which the crawler uses to load and render the specification.

### Crawling a spec

Given the URL of a spec, the crawler basically goes through the following steps:

1. If the URL looks like `http(s)://www.w3.org/TR/[something]`, the crawler extracts the shortname of the specification, and sends a couple of requests to the W3C API to retrieve the URL of the Editor's Draft, or the URL of the latest published version if the URL of the Editor's Draft could not be found. This new URL replaces the given one.
2. Fetch the URL. Note Reffy uses a network cache on the local filesystem, and sends conditional HTTP requests if the URL is already in that cache
3. Render the response with jsdom, which should create a `Window` object. Note rendering with jsdom may trigger additional fetches (e.g. to retrieve scripts), which also go through the network cache.
4. If needed, wait until the document is properly generated. This is typically needed for specs written with ReSpec that are generated on-the-fly.
5. Run internal tools on the generated document to build the relevant information.

The crawler processes 10 specification at a time. It may crash from time to time, e.g. because of network errors. Beware: errors are not properly reported yet.

### Config parameters

The crawler reads parameters from the `config.json` file. To be able to interact with the W3C API, that file must contain a `w3cApiKey` entry whose value is a valid W3C API Key.

Optional parameters:

* `avoidNetworkRequests`: set this flag to `true` to tell the crawler to use the cache entry for a URL directly, instead of sending a conditional HTTP request to check whether the entry is still valid. This parameter is typically useful when developing Reffy's code to work offline.
* `resetCache`: set this flag to `true` to tell the crawler to reset the contents of the local cache when it starts.

## Contributing

Main authors so far are [François Daoust](https://github.com/tidoust/) and [Dominique Hazaël-Massieux](https://github.com/dontcallmedom/).

Additional ideas, bugs and/or code contributions are most welcome. Create [issues on GitHub](https://github.com/tidoust/issues) as needed!


## Licensing

The code is available under an [MIT license](LICENSE).